[
  {
    "objectID": "assignment.html",
    "href": "assignment.html",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "A route-based spatial analysis of bird species richness across the contiguous United States, using The North American Breeding Bird Survey (BBS) data and combination of environmental (NDVI) and spatial (elevation, Lon, Lat) predictors.\n\n\nOne of the basic questions in biology asks the following: What is the origin of biodiversity? Why are there so many distinct species at the same site? These questions can be examined through evolutionary and ecological perspectives, which are often connected. The ecological approach is crucial, because it links the evolutionary history of coexisting species with the environment in which they occur and examines the factors which allow not just the build-up of local (and consequently regional) species richness, but also the ways it is maintained over time. One of the factors that facilitates species coexistence in birds and therefore drives their species richness (number of species at the site) on the regional scale is primary productivity of the environment (Pigot et al. 2016).\nAt broad spatial scales, species richness typically increases with productivity (Hawkins et al. 2003). In contrast, at finer spatial scales, the relationship between productivity and species richness is more complex: positive, negative, hump-shaped, and U-shaped relationships have all been documented (Mittelbach et al. 2001). There are several hypotheses for this relationship between productivity and species richness, but we still do not know the ultimate mechanism behind it (Di Cecco et al. 2022). We will look at how the productivity of vegetation drives the species richness on the continental scale of North America. For this, I will use the Breeding Bird Survey (BBS) dataset, which is one of the most significant and longest-running “citizen science” programmes in the world. Established in 1966, its primary goal was to monitor the status and trends of North American bird populations on a continental scale. This census operates on the landscape scale as the routes are about 40 km long with the 50 stops where the volunteers count birds. As the measure of primary productivity and environmental heterogeneity of the vegetation, I decided to use the Normalised Difference Vegetation Index (NDVI) which is used as a proxy for primary productivity in both regional (Nieto et al. 2015) and local studies (Brüggeshemke and Fartmann 2025).\n\n\n\n\nHow vegetation density/heterogeneity predicts bird diversity across United States?\nHow spatial predictors explain the patterns of species richness in North American birds?\n\n\n\n\n\nPrepare and explore the data - clean BBS routes, extract NDVI for routes geometry, and get elevation data with earth engine\nEvaluate global and local spatial autocorrelation in species richness and model residuals\nBuild the OLS model and compare it with GWR model\n\n\n\nShow the code\nimport geopandas as gpd\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport httpx\nimport rioxarray\nimport xvec\nimport folium\nimport ee\nimport geemap\nimport esda\nimport contextily\nfrom libpysal import graph\nimport statsmodels.formula.api as smf\nimport mgwr\nfrom mgwr.gwr import GWR\nfrom mgwr.sel_bw import Sel_BW\n\n\n\n\n\n\n\nI chose the lower 48 states of USA as the study area because for them the route geometries are publicly available. In the basic dataset of BBS, the geometry is included only for the starting point of the routes, which are on average about 40 km long and therefore the single point does not optimally represent their geometry. Firstly, we need to select the boundary for the study area.\n\n\nShow the code\nus_states = gpd.read_file(\"study_area/cb_2018_us_state_500k.shp\")\nexclude = [\"AK\", \"HI\", \"PR\", \"VI\", \"MP\", \"GU\", \"AS\"] # States outside the area of interest\nus_states = us_states.query(\"STUSPS not in @exclude\") # Accessing the exclude variable outside the query() string\nus_boundary = us_states.dissolve()\n\n# Alternatively you can read it directly from the web:\n# states_url = \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\"\n# r = httpx.get(states_url, verify=False, timeout=None)\n# us_states = gpd.read_file(r.content, engine=\"pyogrio\")\n\n\n\n            \n            \n\n\n\n\n\nNow it is time to load the data about species richness, which consists of the number of species detected at the route. I decided to use data from 2018, which I already prepared in R, and computed species richness for each transect. This data has one geometry column with the coordinates of the starting point of the route. I downloaded the data at the official project site: https://www.sciencebase.gov/catalog/item/52b1dfa8e4b0d9b325230cd9.\n\n\nShow the code\nbbs = gpd.read_file(\"bbs/bbs_routes_SR_2018.gpkg\", engine = \"fiona\")\n\nbbs = bbs.loc[bbs[\"CountryNum\"] == 840] # Select only US transects\nbbs = bbs.loc[bbs[\"StateNum\"] != 3] # Select every state except Alaska\nbbs[\"RTENO\"] = bbs[\"RTENO\"].astype(str).str[3:] # Slice the string (skips the first 3 characters)\n\n\n\n            \n            \n\n\nLet’s load the data for the routes. Unfortunately, the route geometries are often inaccurate and sometimes consists of several fragments, which can cause problems. Therefore, we first need to remove potentially wrong route geometries. The expected route geometry ought to be about 40 km long, so let us filter potential outliers from this value. This cleaning will result in the loss of several hundred datapoints, but we need precise route geometries for data extraction.\n\n\nShow the code\nroutes = gpd.read_file(\"routes/bbsrtsl020.shp\")\n\nroutes = routes.to_crs(epsg=5070) # First set the crs to USA Contiguous Albers Equal Area Conic projection\nroutes[\"calc_length_km\"] = routes.geometry.length / 1000 # Calculating the route length for given geometry\nroutes = routes.loc[routes[\"RTELENG\"] &lt;= 45000,] # Selecting routes under 45 km in length by the dataset measure\nmask = (routes[\"calc_length_km\"] &gt;= 35) & (routes[\"calc_length_km\"] &lt;= 45) # Filter out fragments and over-long routes\nroutes_filtered = routes[mask].copy()\n\nprint(f\"Original routes count: {len(routes)}\")\nprint(f\"Filtered routes count (35-45km): {len(routes_filtered)}\")\nprint(f\"Average route length: {routes_filtered[\"calc_length_km\"].mean():.2f} km\")\n\n\n\n            \n            \n\n\nOriginal routes count: 3062\nFiltered routes count (35-45km): 2570\nAverage route length: 40.14 km\n\n\nLet us perform the spatial join of these two geodataframes. To combine BBS data with routes geometries we use spatial join using “inner” as we need matching values in both tables. I could also join the tables by column “RTENO”, but I want to be sure that the geometries match.\n\n\nShow the code\nbbs = bbs.to_crs(routes_filtered.crs) # Make sure it's in the same coordinate system\nbbs = bbs.drop(columns = \"RTENO\")\n\nbbs[\"geometry\"] = bbs.buffer(400) # Add a small buffer (400 m) to points to account for GPS inaccuracy\n\njoined_data = gpd.sjoin(routes_filtered, bbs, how=\"inner\", predicate=\"intersects\") # Spatial join of bbs points data with the routes geometries\n\nprint(f\"Routes in subset: {len(routes_filtered)}\")\nprint(f\"Routes with data: {len(joined_data[\"RTENO\"].unique())}\")\n\n\n\n            \n            \n\n\nRoutes in subset: 2570\nRoutes with data: 1611\n\n\nThe large drop in the number of routes is due to missing BBS data for the route geometries, which could be due to annual differences in the census - in some years, particular routes are not counted due to weather conditions, terrain obstacles or illness of the observer. However, the number is still very large, so there are probably some other errors and inaccuracies in geometry matching. Now we will do the final preparation of the table for the NDVI and elevation values extraction. We need to get rid of redundant columns, create buffer around the routes, add the route midpoint and create Lat/Lon columns for external API.\n\n\nShow the code\n# Add .copy() here to break the link to joined_data\nbbs_routes = joined_data[[\"RTENO\", \"species_richness\", \"geometry\"]].copy()\n\nbbs_routes[\"buff_3000\"] = bbs_routes.buffer(3000)  # Creating 3 km buffer - buffer choice explained in NDVI extraction part\nbbs_routes[\"route_midpoint\"] = bbs_routes.geometry.interpolate(0.5, normalized=True) # Calculate the midpoint (50% along the line) as a reference geometry\nbbs_routes = bbs_routes.drop(columns = \"geometry\") # We no longer need routes geometry\n\ntemp_gdf = bbs_routes.set_geometry(\"route_midpoint\").to_crs(4326)\nbbs_routes[\"lon\"] = temp_gdf.geometry.x\nbbs_routes[\"lat\"] = temp_gdf.geometry.y\nbbs_routes = bbs_routes.set_geometry(\"buff_3000\")\nbbs_routes.head()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nRTENO\nspecies_richness\nbuff_3000\nroute_midpoint\nlon\nlat\n\n\n\n\n0\n2072\n63\nPOLYGON ((782767.664 1352804.909, 782613.87 13...\nPOINT (799058.576 1358112.615)\n-87.165585\n34.947661\n\n\n2\n2204\n54\nPOLYGON ((911518.247 1350136.581, 911484.644 1...\nPOINT (920348.359 1362929.663)\n-85.828469\n34.882272\n\n\n3\n2073\n65\nPOLYGON ((883221.105 1343004.45, 883424.384 13...\nPOINT (896739.418 1353510.697)\n-86.098287\n34.820791\n\n\n4\n2071\n61\nPOLYGON ((766029.847 1327436.538, 765484.087 1...\nPOINT (776778.876 1331128.227)\n-87.437364\n34.725655\n\n\n5\n2007\n63\nPOLYGON ((869910.569 1340151.781, 869882.814 1...\nPOINT (879587.76 1341112.337)\n-86.300271\n34.726327\n\n\n\n\n\n\n\n\n\n\nThe normalised difference vegetation index (NDVI) indicates vegetation health within raster image pixels by quantifying how much near-infrared (NIR) light the plants reflect. Healthy vegetation reflects a higher proportion of NIR and a lower proportion of red light than stressed vegetation or non-living surfaces with similar visible colours (for example, turf fields). This contrast makes NDVI a practical tool for evaluating how healthy vegetation is in a raster image relative to its surroundings.\nNDVI is calculated for each pixel with the following calculation:\n\\(\\Large NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\)\nThis formula generates a value between -1 and +1. Low reflectance in the red channel and high reflectance in the NIR channel will yield a high NDVI value (healthy vegetation), while the inverse will result in a low NDVI value (unhealthy vegetation). Negative values typically represent non-vegetation such as water or rock.\nThe NDVI raster for the whole area would be very large (probably several Gb) and computationally demanding. Therefore, I decided to choose a different approach and work with compressed 8-bit raster (values from 0 to 255) from NEO (Nasa Earth Observations), which is a 1-month average (June). According to NASA Earth Observations (NEO) (2026), the values contained in this file have been scaled and resampled to support visualisation in NEO and are not suitable for rigorous scientific analysis. However, they may be used for basic assessments and for identifying general trends, which is exactly what we are going to do. We only need to resolve the different scaling of the values. We will extract the mean NDVI values for 3 km buffer. these distances preserve the higher predictive power for covariates in studies of breeding birds (e.g. Byer et al. 2025).\n\n\nShow the code\nndvi = rioxarray.open_rasterio(\"MOD_NDVI_M_2018-06-01_rgb_3600x1800.TIFF\")\n\nndvi_us = ndvi.rio.clip(us_boundary.to_crs(ndvi.rio.crs).geometry) # Matching CRS and cliping to study area\nndvi_us = ndvi_us.drop_vars(\"band\").squeeze() # Getting rid of redundant dimension\nndvi_us = ndvi_us.where(ndvi_us&gt;0) # Filtering null values\n\nprint(f\"Raster CRS: {ndvi_us.rio.crs}\") # Check the CRS to see if it's in degrees (4326) or meters\n\n\n\n            \n            \n\n\nRaster CRS: EPSG:4326\n\n\n\n\nShow the code\n_ = ndvi_us.plot()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe can see from the map that there is a very high NDVI on the east coast compared to the west of US. Now we can extract values from the raster for the routes buffer we created earlier. We will extract 1) the mean value of NDVI for each buffer and 2) the standard deviation of NDVI for each buffer. The latter will help estimate the heterogeneity of the vegetation.\n\n\nShow the code\nndvi_us = ndvi_us.rio.reproject(bbs_routes.crs) # Reprojecting to EPSG: 5070\nndvi_extraction = bbs_routes.to_crs(ndvi_us.rio.crs)\n\nzonal_stats = ndvi_us.drop_vars(\"spatial_ref\").xvec.zonal_stats(\n    geometry=ndvi_extraction.buff_3000,\n    x_coords=\"x\",\n    y_coords=\"y\",\n    stats=[\"mean\", \"std\"],\n)\n\n\n\n            \n            \n\n\nLet us check if there are some missing values in the data.\n\n\nShow the code\nndvi = zonal_stats.xvec.to_geodataframe(name=\"NDVI\") # Tranforming zonal statistics into geodataframe\nprint(ndvi.isna().sum()) # Returns the count of NaNs for every column in the dataframe\n\n\n\n            \n            \n\n\ngeometry     0\nindex        0\nNDVI        66\ndtype: int64\n\n\n\n\nShow the code\nndvi_clean = ndvi.dropna(subset=[\"NDVI\"]) # Dropping rows where the NDVI value is missing\n\nndvi_stats = ndvi_clean.reset_index().pivot(\n    index=\"index\", \n    columns=\"zonal_statistics\", \n    values=\"NDVI\"\n) # This creates a table with index as rows and mean/std/min/max as columns\n\nndvi_stats.columns = [\"ndvi_\" + col for col in ndvi_stats.columns] # Add the \"ndvi_\" prefix to the new columns\ngeometries = ndvi_clean[[\"index\", \"geometry\"]].drop_duplicates(\"index\").set_index(\"index\") # Get the unique geometries for each index\nndvi_data = geometries.join(ndvi_stats).reset_index() # Join them together\n\n\n\n            \n            \n\n\nAlthough the NDVI data was retrieved in 8-bit format (0–255), we can standardise it to Z-scores prior to modelling. This process centres the data and removes the original units, ensuring that the coefficients in the models are not biassed by the original bit-depth of the satellite imagery.\n\n\nShow the code\n# Standardize the Mean (Relative Productivity)\n# (Value - average of all routes) / standard deviation of all routes\nndvi_data[\"ndvi_mean_z\"] = (ndvi_data[\"ndvi_mean\"] - ndvi_data[\"ndvi_mean\"].mean()) / ndvi_data[\"ndvi_mean\"].std()\n\n# Standardize the Std (Relative Heterogeneity)\n# We treat the \"std\" as its own variable and standardize IT.\nndvi_data[\"ndvi_heterog_z\"] = (ndvi_data[\"ndvi_std\"] - ndvi_data[\"ndvi_std\"].mean()) / ndvi_data[\"ndvi_std\"].std()\n\n\n\n            \n            \n\n\n\n\nShow the code\nndvi_to_join = ndvi_data[[\"ndvi_mean_z\", \"ndvi_heterog_z\", \"geometry\"]]\n\nbuffer = bbs_routes.set_geometry(\"buff_3000\")\n\n# Perform the Spatial Join\n# \"predicate=inner\" ensures the polygons must overlap/be the same\n# \"how=inner\" keeps only rows that exist in both sets\nbbs_ndvi = gpd.sjoin(\n    buffer, \n    ndvi_to_join, \n    how=\"inner\", \n    predicate=\"intersects\"\n)\n\nbbs_ndvi = bbs_ndvi.drop(columns=\"index_right\") # Dropping index_right\ndata = bbs_ndvi.drop_duplicates(subset=[\"RTENO\"]) # Check for duplicates\ndata.info()\n\n\n\n            \n            \n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 1592 entries, 0 to 3730\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   RTENO             1592 non-null   int64   \n 1   species_richness  1592 non-null   int64   \n 2   buff_3000         1592 non-null   geometry\n 3   route_midpoint    1592 non-null   geometry\n 4   lon               1592 non-null   float64 \n 5   lat               1592 non-null   float64 \n 6   ndvi_mean_z       1592 non-null   float64 \n 7   ndvi_heterog_z    1592 non-null   float64 \ndtypes: float64(4), geometry(2), int64(2)\nmemory usage: 111.9 KB\n\n\n\n\nShow the code\nm = data.explore(\n    column=\"ndvi_mean_z\", \n    cmap=\"RdYlGn\",\n    marker_kwds={'radius':5},\n    vmin=-2.5, \n    vmax=2.5, \n    legend=True,\n    tooltip=[\"ndvi_mean_z\"], # Hover to see raw vs z-score\n    tiles=\"CartoDB positron\",\n    popup=True # Click to see all data for that route\n)\nm\n\n\n\n            \n            \n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThis map displays the relative differences in the NDVI standardised in the US. A visible challenge for our spatial model is the high density and overlap of the routes in the Northeast. This concentration introduces sampling bias (over-representing certain environments) and pseudo replication, where overlapping buffers sample nearly identical pixels, violating the assumption of independent observations.\nTo mitigate this, I implemented a spatial thinning procedure using the function thin_spatial_geopandas. This algorithm utilises a spatial index (sindex) to efficiently identify route buffers within a 1 m threshold and randomly selects one representative route from overlapping pairs to be retained, while discarding the rest. This ensures a more geographically balanced dataset for subsequent analysis.\n\n\nShow the code\ndef thin_spatial_geopandas(data, distance_threshold=1):\n    # 1. Ensure we are working with a copy and shuffle for randomness\n    gdf_temp = data.copy().sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    keep_indices = []\n    discard_indices = []\n    processed = np.zeros(len(gdf_temp), dtype=bool)\n\n    # 2. Build a spatial index\n    sindex = gdf_temp.sindex\n\n    for i in range(len(gdf_temp)):\n        if processed[i]:\n            continue\n        \n        # Keep this route\n        keep_indices.append(i)\n        processed[i] = True\n        \n        # Find all neighbors within the distance threshold\n        current_geom = gdf_temp.geometry.iloc[i]\n        # query returns indices of geometries whose bounding box intersects the search area\n        # Then we filter by actual distance\n        potential_neighbors = sindex.query(current_geom.buffer(distance_threshold))\n        \n        for neighbor_idx in potential_neighbors:\n            if not processed[neighbor_idx] and neighbor_idx != i:\n                # Double check actual distance (since query uses bounding boxes)\n                if current_geom.distance(gdf_temp.geometry.iloc[neighbor_idx]) &lt; distance_threshold:\n                    discard_indices.append(neighbor_idx)\n                    processed[neighbor_idx] = True\n                    \n    return gdf_temp.iloc[keep_indices], gdf_temp.iloc[discard_indices]\n\n# Run the thinning (make sure your CRS is in meters, e.g., EPSG:5070)\ndf_thinned, df_removed = thin_spatial_geopandas(data, distance_threshold=1)\n\nprint(f\"Routes kept: {len(df_thinned)}\")\nprint(f\"Routes removed: {len(df_removed)}\")\n\n\n\n            \n            \n\n\nRoutes kept: 1442\nRoutes removed: 150\n\n\n\n\nShow the code\nbbs_ndvi = df_thinned.drop(columns = \"buff_3000\") # We no longer need buffer geometry\nbbs_ndvi = bbs_ndvi.set_geometry(\"route_midpoint\")\nbbs_ndvi.info()\n\n\n\n            \n            \n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 1442 entries, 0 to 1591\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   RTENO             1442 non-null   int64   \n 1   species_richness  1442 non-null   int64   \n 2   route_midpoint    1442 non-null   geometry\n 3   lon               1442 non-null   float64 \n 4   lat               1442 non-null   float64 \n 5   ndvi_mean_z       1442 non-null   float64 \n 6   ndvi_heterog_z    1442 non-null   float64 \ndtypes: float64(4), geometry(1), int64(2)\nmemory usage: 90.1 KB\n\n\n\n\n\nTo get the elevation data, I integrated topographic data from the NASA Digital Elevation Model (NASADEM). Since the Breeding Bird Survey data do not inherently contain altitude information, I used the Google Earth Engine (GEE) API to perform cloud-based spatial join operations.\nThe extraction process involved several technical steps:\nFeature Transformation: Local coordinate pairs (Latitude/Longitude) for each survey route midpoint were converted into ee.Feature objects to make them compatible with the GEE spatial engine. - High-Resolution Sampling: I accessed the NASADEM Global 30m dataset. Using a 30-metre sampling scale, the extraction matched the native resolution of the satellite sensor, ensuring that the elevation value assigned to each route accurately reflects the local terrain. - Data Integration: The extracted values were pulled from the cloud using the geemap library and merged back into the primary research DataFrame using the unique route identifier (RTENO).\n\n\nShow the code\n# 1. Initialize Earth Engine Project ID\ntry:\n    ee.Initialize(project=\"my-project-ee-470714\")\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize(project=\"my-project-ee-470714\")\n\n# 2. Convert DataFrame rows into GEE Features\nfeatures = []\nfor index, row in bbs_ndvi.iterrows():\n    # GEE uses [Longitude, Latitude]\n    point = ee.Geometry.Point([row[\"lon\"], row[\"lat\"]])\n    feat = ee.Feature(point, {\"RTENO\": row[\"RTENO\"]})\n    features.append(feat)\n\npoints_ee = ee.FeatureCollection(features)\n\n# 3. Load the NASADEM Global 30m dataset\nnasadem = ee.Image(\"NASA/NASADEM_HGT/001\").select(\"elevation\")\n\n# 4. Sample the image at road midpoints\nsampled_points = nasadem.sampleRegions(\n    collection=points_ee,\n    properties=[\"RTENO\"],\n    scale=30\n)\n\n# 5. Convert back to a Pandas DataFrame\n# 5. Convert back to a Pandas DataFrame (The Direct Way)\ninfo = sampled_points.getInfo()\nfeatures = info['features']\ndict_list = [f['properties'] for f in features]\ndf_elev = pd.DataFrame(dict_list)\n# 6. Merge the elevation into final DataFrame\ndata_final = bbs_ndvi.merge(df_elev[[\"RTENO\", \"elevation\"]], on=\"RTENO\", how=\"left\")\n\nprint(\"Elevation successfully extracted from GEE!\")\n\n\n\n            \n            \n\n\nElevation successfully extracted from GEE!\n\n\n\n\nShow the code\ndata_final[\"elevation\"].describe()\n\n\n\n            \n            \n\n\ncount    1442.000000\nmean      592.497226\nstd       648.428474\nmin        -1.000000\n25%       162.000000\n50%       314.000000\n75%       816.750000\nmax      3289.000000\nName: elevation, dtype: float64\n\n\n\n\nShow the code\n# Save to a GeoPackage\n# Pass the column names as a list\ndata_final.to_file(\"assignment_data.gpkg\", layer=\"bbs_routes\", driver=\"GPKG\")\n\n\n\n            \n            \n\n\n\n\nShow the code\ndata = gpd.read_file(\"assignment_data.gpkg\")\ndata.head()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nRTENO\nspecies_richness\nlon\nlat\nndvi_mean_z\nndvi_heterog_z\nelevation\ngeometry\n\n\n\n\n0\n60130\n26\n-103.354305\n32.340717\n-2.006735\n-0.573757\n1073\nPOINT (-687991.066 1056389.17)\n\n\n1\n72016\n65\n-78.818844\n41.692045\n1.028855\n-0.141942\n508\nPOINT (1411937.291 2204439.401)\n\n\n2\n91036\n55\n-90.357021\n44.651087\n0.146810\n0.078148\n369\nPOINT (446389.92 2420135.718)\n\n\n3\n34068\n47\n-89.311522\n41.466289\n1.044674\n-0.629305\n221\nPOINT (553981.276 2070695.428)\n\n\n4\n91023\n56\n-89.648401\n45.779301\n0.873833\n-0.629305\n502\nPOINT (493986.861 2548870.791)\n\n\n\n\n\n\n\n\n\n\n\nWith the data prepared, we first evaluate the extent of spatial dependency in bird species richness using a three-pronged diagnostic approach: - Moran plot - to see the global degree of clustering - Correlogram - to see how the spatial autocorrelation changes with larger neighbourhoods - LISA (Local Indicators of Spatial Association) - to see distribution of significantly spatialy autocorrelated values.\n\n\nShow the code\n# 1. Prepare standardized variables and spatial lags\nknn5 = graph.Graph.build_knn(data, k=5)\nknn5 = knn5.transform(\"R\")\n# Standardize richness (Z-score)\ndata[\"richness_std\"] = (data[\"species_richness\"] - data[\"species_richness\"].mean()) / data[\"species_richness\"].std()\n# Calculate spatial lag using the KNN5 weights you already built\ndata[\"richness_lag\"] = knn5.lag(data[\"richness_std\"])\n# Calculate Moran's I\nmoran = esda.moran.Moran(data['species_richness'], knn5)\n# Calculate correlogram\nk = [5, 10, 25, 50, 75, 100, 500, 1000]\ncorrelogram = esda.correlogram(\n  geometry=data.representative_point(),\n  variable=data[\"species_richness\"],\n  support=k,\n  distance_type=\"knn\",\n)\n\n# 2. Calculate LISA (Local Indicators of Spatial Association)\n# We use .to_W() because Moran_Local expects a weights object\nlisa = esda.moran.Moran_Local(data[\"species_richness\"], knn5.to_W())\n\n# 3. Setup the 3-panel figure\nf, ax = plt.subplots(1, 3, figsize=(24,7))\n\n# --- PANEL 1: MORAN SCATTERPLOT ---\nsns.regplot(\n    x=\"richness_std\",\n    y=\"richness_lag\",\n    data=data,\n    marker=\".\",\n    scatter_kws={\"alpha\": 0.3, \"color\": \"steelblue\"},\n    line_kws=dict(color=\"lightcoral\"),\n    ax=ax[0]\n)\nax[0].set_aspect(\"equal\")\nax[0].axvline(0, c=\"black\", alpha=0.5, linewidth=1)\nax[0].axhline(0, c=\"black\", alpha=0.5, linewidth=1)\nax[0].set_title(f\"Moran Plot\\n\\nMoran's I: {moran.I:.3f} (p-value: {moran.p_sim:.3f})\", fontsize=14)\nax[0].set_xlabel(\"Species Richness (Standardized)\")\nax[0].set_ylabel(\"Spatial Lag (Standardized)\")\n\n# Add Quadrant Labels\nax[0].text(0.95, 0.95, \"HH\", transform=ax[0].transAxes, ha=\"right\", va=\"top\", fontweight=\"bold\")\nax[0].text(0.95, 0.05, \"HL\", transform=ax[0].transAxes, ha=\"right\", va=\"bottom\", fontweight=\"bold\")\nax[0].text(0.05, 0.95, \"LH\", transform=ax[0].transAxes, ha=\"left\", va=\"top\", fontweight=\"bold\")\nax[0].text(0.05, 0.05, \"LL\", transform=ax[0].transAxes, ha=\"left\", va=\"bottom\", fontweight=\"bold\")\n\n# --- PANEL 2: LISA CLUSTER MAP ---\n_ = lisa.plot(data, crit_value=0.05, ax=ax[1], alpha=0.6, markersize=20) \nax[1].set_title(\"LISA Cluster Map (p &lt; 0.05)\", fontsize=14)\ncontextily.add_basemap(ax[1], crs=data.crs.to_string(), source=contextily.providers.CartoDB.PositronNoLabels)\n\n# --- PANEL 3: CORRELOGRAM ---\n# Plotting the I values against the support (K neighbors)\nax[2].plot(k, correlogram.I, marker=\"o\", linestyle=\"-\", color=\"steelblue\")\nax[2].axhline(0, color=\"black\", linestyle=\"--\", alpha=0.5)\nax[2].set_title(\"Spatial Correlogram\", fontsize=14)\nax[2].set_xlabel(\"Number of Nearest Neighbors (K)\")\nax[2].set_ylabel(\"Moran's I\")\nax[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nThe Global Moran’s I value confirms a strong, positive spatial autocorrelation, indicating that the species richness is geographically clustered. The LISA map reveals a distinct East-West dichotomy: the Eastern US is dominated by biodiversity hotspots (High-High), likely driven by higher precipitation and vegetation density, while the arid West exhibits extensive coldspots (Low-Low).\nAn interesting anomaly is the coldspot cluster in Florida. While Florida is ecologically rich in wetlands, so I would not expect a clustering of lower species richness values.\nFinally, the spatial correlogram demonstrates that spatial influence is strongest at small scales but drops significantly until the neighbourhood size reaches approximately 100 neighbours. This inflexion point suggests that the ecological processes driving bird richness operate within a specific regional range.\n\n\n\n\n\nNow we can continue with linear regression. We already know, that there is a high spatial autocorrelation, therefore our ultimate goal is geographically weighted linear regression (GWR), but first we will build simple OLS model and identify the residuals. However this model will already consist of two spatial predictors - longitude and latitude.\n\n\nShow the code\n# Define our dependent variable (target) and independent variables (predictors)\ndependent = \"species_richness\"\nindependents = [\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\", \"lon\", \"lat\"]\n\n# Construct the formula string: \"species_richness ~ ndvi_mean_z + ndvi_heterog_z + elevation\"\nformula = f\"{dependent} ~ {\" + \".join(independents)}\"\nprint(f\"OLS Formula: {formula}\")\n\n\n\n            \n            \n\n\nOLS Formula: species_richness ~ ndvi_mean_z + ndvi_heterog_z + elevation + lon + lat\n\n\n\n\nShow the code\n# Fit the Ordinary Least Squares (OLS) model\nols = smf.ols(formula, data=data).fit()\n\nols.summary()\n\n\n\n            \n            \n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nspecies_richness\nR-squared:\n0.329\n\n\nModel:\nOLS\nAdj. R-squared:\n0.327\n\n\nMethod:\nLeast Squares\nF-statistic:\n141.1\n\n\nDate:\nFri, 13 Feb 2026\nProb (F-statistic):\n6.76e-122\n\n\nTime:\n14:09:35\nLog-Likelihood:\n-5542.3\n\n\nNo. Observations:\n1442\nAIC:\n1.110e+04\n\n\nDf Residuals:\n1436\nBIC:\n1.113e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n51.1524\n3.096\n16.521\n0.000\n45.079\n57.226\n\n\nndvi_mean_z\n5.8241\n0.449\n12.976\n0.000\n4.944\n6.705\n\n\nndvi_heterog_z\n1.3671\n0.313\n4.368\n0.000\n0.753\n1.981\n\n\nelevation\n-0.0015\n0.001\n-2.415\n0.016\n-0.003\n-0.000\n\n\nlon\n0.1071\n0.029\n3.693\n0.000\n0.050\n0.164\n\n\nlat\n0.3419\n0.066\n5.201\n0.000\n0.213\n0.471\n\n\n\n\n\n\n\n\nOmnibus:\n10.586\nDurbin-Watson:\n1.967\n\n\nProb(Omnibus):\n0.005\nJarque-Bera (JB):\n11.957\n\n\nSkew:\n0.141\nProb(JB):\n0.00253\n\n\nKurtosis:\n3.346\nCond. No.\n9.16e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 9.16e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe Global OLS model indicates that primary productivity (ndvi_mean_z) is the dominant driver of bird species richness across the contiguous US and the model explains about one third of the data (R2=0.329). Bird diversity slightly decreases with higher elevation and increases with vegetation heterogeneity. Longitude and latitude have very small explanatory power. Furthermore, the OLS model assumes these relationships are constant across the continent. To investigate whether these drivers vary geographically—for instance, if NDVI is more important in the arid West than the humid East—I will now proceed to Geographically Weighted Regression (GWR). But first we take a look at the spatial perspective of the OLS model.\n\n\nShow the code\npredicted = ols.predict(data)\ndata[\"residual\"] = ols.resid\nmax_residual = ols.resid.abs().max()\n\nf, axs = plt.subplots(1, 3, figsize=(24, 7))\ndata.plot(\n    predicted, legend=True, cmap=\"coolwarm\", vmin=0, vmax=100, ax=axs[0], alpha = 0.8, markersize=30\n)\ndata.plot(\n    \"species_richness\", legend=True, cmap=\"coolwarm\", vmin=0, vmax=100, ax=axs[1], alpha = 0.8, markersize=30\n)\n\ndata.plot(\n    \"residual\", legend=True, cmap=\"RdBu\", vmin=-max_residual, vmax=max_residual, ax=axs[2], alpha = 0.8, markersize=30\n)\n\nres_moran = esda.moran.Moran(data[\"residual\"], knn5)\n\n\naxs[0].set_title(\"OLS prediction\", fontsize=14)\naxs[1].set_title(\"Actual results\", fontsize=14)\naxs[2].set_title(f\"Residuals\\n\\nMoran's I: {res_moran.I:.3f} (p-value: {res_moran.p_sim:.3f})\", fontsize=14)\n\naxs[0].set_axis_off()\naxs[1].set_axis_off()\naxs[2].set_axis_off()\nplt.tight_layout()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nIn the first map we can observe that the model predicts the expected difference in the species richness between East and West of US. Hence, it both overestimates and underestimates the predictions in some areas, because our data (middle map) shows more diverse pattern. The map on the right present’s residuals, locations where models fail to explain our dependent variable. You can spot the blue areas which have higher species richness than expected by the model, while the red areas show the opposite (including Florida, which has lower species diversity than expected). Moran’s I indicates that residuals form spatial clusters.\n\n\n\n\n\nShow the code\n# 1. Define Dependent (y) and Independent (X) variables\n# We reshape y to (-1, 1) to ensure it is a column vector for matrix math\ny = data[\"species_richness\"].values.reshape((-1, 1))\n\n# We select our environmental predictors\n# We exclude lon/lat from X because they are used in \"coords\" instead\nX = data[[\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\"]].values\n\n# 2. Define the spatial coordinates\ncoords = data.centroid.get_coordinates().values\n\n\n\n            \n            \n\n\n\n\nShow the code\n# Initialize the bandwidth selector\nbw_selector = Sel_BW(coords, y, X, fixed=False, multi=False)\n\n# Search for the optimal bandwidth (number of nearest neighbors)\nopt_bw = bw_selector.search()\n\nprint(f\"Optimal bandwidth (neighbors): {opt_bw}\")\n\n\n\n            \n            \n\n\nOptimal bandwidth (neighbors): 80.0\n\n\n\n\nShow the code\n# Fit the model\ngwr_model = GWR(coords, y, X, opt_bw).fit()\n\n# View the global summary of the local models\ngwr_model.summary()\n\n\n\n            \n            \n\n\n===========================================================================\nModel type                                                         Gaussian\nNumber of observations:                                                1442\nNumber of covariates:                                                     4\n\nGlobal Regression Results\n---------------------------------------------------------------------------\nResidual sum of squares:                                         188213.517\nLog-likelihood:                                                   -5558.494\nAIC:                                                              11124.988\nAICc:                                                             11127.030\nBIC:                                                             177753.813\nR2:                                                                   0.314\nAdj. R2:                                                              0.313\n\nVariable                              Est.         SE  t(Est/SE)    p-value\n------------------------------- ---------- ---------- ---------- ----------\nX0                                  54.352      0.458    118.587      0.000\nX1                                   7.121      0.383     18.574      0.000\nX2                                   1.381      0.313      4.420      0.000\nX3                                  -0.001      0.001     -2.067      0.039\n\nGeographically Weighted Regression (GWR) Results\n---------------------------------------------------------------------------\nSpatial kernel:                                           Adaptive bisquare\nBandwidth used:                                                      80.000\n\nDiagnostic information\n---------------------------------------------------------------------------\nResidual sum of squares:                                         122138.628\nEffective number of parameters (trace(S)):                          160.282\nDegree of freedom (n - trace(S)):                                  1281.718\nSigma estimate:                                                       9.762\nLog-likelihood:                                                   -5246.719\nAIC:                                                              10816.001\nAICc:                                                             10856.906\nBIC:                                                              11666.566\nR2:                                                                   0.555\nAdjusted R2:                                                          0.499\nAdj. alpha (95%):                                                     0.001\nAdj. critical t value (95%):                                          3.234\n\nSummary Statistics For GWR Parameter Estimates\n---------------------------------------------------------------------------\nVariable                   Mean        STD        Min     Median        Max\n-------------------- ---------- ---------- ---------- ---------- ----------\nX0                       55.634      9.885     19.849     55.695     85.198\nX1                        4.078      5.228    -14.156      4.573     16.293\nX2                        1.041      2.064     -6.011      1.245      6.923\nX3                        0.002      0.037     -0.083     -0.002      0.200\n===========================================================================\n\n\n\nThe GWR model increased the explained variability to 55 % and we can now do a visual comparison with the OLS model and actual data.\n\n\nShow the code\n# Create a copy to store results\ndata_results = data.copy()\n\n# Add Local R2 (Where does the model perform best?)\ndata_results['gwr_R2'] = gwr_model.localR2\n\n# Add Local Coefficients\n# Since X had [ndvi_mean_z, ndvi_heterog_z, elevation], the params match that order\ndata_results['coef_ndvi'] = gwr_model.params[:, 0]\ndata_results['coef_hetero'] = gwr_model.params[:, 1]\ndata_results['coef_elev'] = gwr_model.params[:, 2]\n\n# Add T-values (to check local significance)\n# A t-value &gt; 1.96 or &lt; -1.96 means the relationship is significant at 95%\ndata_results['t_ndvi'] = gwr_model.tvalues[:, 0]\n\n\n\n            \n            \n\n\n\n\nShow the code\n# 1. Define the maximum richness for a consistent scale\nv_min = 0\nv_max = 100 \n\nfig, axs = plt.subplots(1, 3, figsize=(24, 7))\n\n# --- Plot 1: OLS Prediction ---\n# We use the global OLS model to predict based on the dataframe\ndata.plot(\n    ols.predict(data), \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[0], markersize=40\n)\naxs[0].set_title(\"OLS Prediction (Global Model)\", fontsize=14)\n\n# --- Plot 2: GWR Prediction ---\n# 'predy' contains the local predictions from the GWR model\ndata.plot(\n    gwr_model.predy.flatten(), \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[1], markersize=40\n)\naxs[1].set_title(\"GWR Prediction (Local Model)\", fontsize=14)\n\n# --- Plot 3: Actual Data ---\n# The ground truth: your 'species_richness' column\ndata.plot(\n    \"species_richness\", \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[2], markersize=40\n)\naxs[2].set_title(\"Actual Species Richness\", fontsize=14)\n\n# Cleanup\nfor ax in axs:\n    ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nAs we can see from the maps, the GWR model better explains the variability on the actual data further highliting the contrast between East and West, however still fails to predicts very high or very low values in some areas - to look at them specifically, we can examine the local Beta coefficients.\n\n\nShow the code\n# 1. Define significance threshold\nsig95 = gwr_model.adj_alpha[1]\ncritical_t = gwr_model.critical_tval(alpha=sig95)\ncritical_t\nsignificant = np.abs(gwr_model.tvalues) &gt; critical_t\n\n# 2. Get the actual number of variables from the model\nnum_vars = gwr_model.params.shape[1]\n\n# 3. Build var_names safely\n# 'independents' usually includes the target or coords, so we slice it \n# to match the X matrix you fed into the GWR\n# If your X was [ndvi_mean_z, ndvi_heterog_z, elevation], then:\nactual_predictor_names = [\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\"]\nvar_names = [\"Intercept\"] + actual_predictor_names\n\n# 4. Create the grid\ncols = 2\nrows = (num_vars + cols - 1) // cols \n\nfig, axs = plt.subplots(rows, cols, figsize=(12, rows * 5))\naxs = axs.flatten()\n\nfor i in range(num_vars):\n    # Plot the local coefficients\n    data.plot(\n        gwr_model.params[:, i], \n        cmap=\"coolwarm\", \n        ax=axs[i], \n        markersize=15, # Increased size for better visibility\n        legend=True,\n        legend_kwds={\"shrink\": 0.5}\n    )\n    \n    # Significance Mask\n    non_sig_mask = ~significant[:, i]\n    if non_sig_mask.any():\n        data[non_sig_mask].plot(\n            color=\"white\", \n            ax=axs[i], \n            alpha=0.7, \n            markersize=15\n        )\n    \n    # Check if we have a name for this index to avoid IndexError\n    title_name = var_names[i] if i &lt; len(var_names) else f\"Variable {i}\"\n    axs[i].set_title(f\"Local Coef: {title_name}\", fontsize=12)\n    axs[i].set_axis_off()\n\n# Hide any extra axes\nfor j in range(i + 1, len(axs)):\n    axs[j].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nThe intercept coefficient shows the species richness that was not explained by the model. We can see clusters of species poor routes in the rain shadow of the Rocky Mountains and Great Basin, while biodiversity hotpost are situated mostly in the area of Great Lakes. These patterns, not captured by the model, could be also explained by combination of factors like regional history, presence of nature protective areas and human influence. Relative NDVI significantly drives bird diversity in the arid west, especially in desert habitats, where only a slight increase in vegetation can result in a more diverse bird community. The heterogeneity of the vegetation is significant only in the Great Basin and has not a very strong effect. Finally, in Florida, elevation implies the explanation of species richness, but this is very probably not the right prediction as Florida has very low elevation heterogeneity and it only significantly differs from the other states in the average elevation, but this is surely not the driver of bird diversity in this area. The generally lower species diversity of Florida Peninsula in the model is driven by some factors which were not included - maybe high human population density or climate unstability.\n\n\n\n\nThis study confirms that bird species richness is driven by spatially varying processes. Global Moran’s I and LISA maps reveal a sharp East-West dichotomy, with the East acting as a biodiversity hotspot and the West as a coldspot. While OLS models capture broad trends, GWR significantly improves accuracy by accounting for local “non-stationarity.”\nLocal coefficients show that NDVI is a vital driver in the arid West, where marginal greenness spikes diversity. Despite the temporarly limiting data and proxy variable for NDVI, the model sufficiently revealed the expected SR~NDVI relationship, but should be treated as explorational with higly limiting significance of the results.\n\n\n\nBrüggeshemke, Jonas, and Thomas Fartmann. 2025. ‘Predicting Species Richness and Abundance of Breeding Birds by Remote‐sensing and Field‐survey Data’. Ecological Solutions and Evidence 6 (4): e70170. https://doi.org/10.1002/2688-8319.70170.\nByer, Nathan W., Remington J. Moll, Timothy J. Krynak, et al. 2025. ‘Breeding Bird Sensitivity to Urban Habitat Quality Is Multi‐scale and Strongly Dependent on Migratory Behavior’. Ecological Applications 35 (1): e3087. https://doi.org/10.1002/eap.3087.\nDi Cecco, Grace J., Sara J. Snell Taylor, Ethan P. White, and Allen H. Hurlbert. 2022. ‘More Individuals or Specialized Niches? Distinguishing Support for Hypotheses Explaining Positive Species–Energy Relationships’. Journal of Biogeography 49 (9): 1629–39. https://doi.org/10.1111/jbi.14459.\nFleischmann, M. (2024) A course on Spatial Data Science. Available at: https://martinfleischmann.net/sds/.\nHawkins, Bradford A., Eric E. Porter, and José Alexandre Felizola Diniz-Filho. 2003. ‘Productivity and History as Predictors of the Latitudinal Diversity Gradient of Terrestrial Birds’. Ecology 84 (6): 1608–23. https://doi.org/10.1890/0012-9658(2003)084%255B1608:PAHAPO%255D2.0.CO;2.\nMittelbach, Gary G., Christopher F. Steiner, Samuel M. Scheiner, et al. 2001. ‘What Is the Observed Relationship Between Species Richness and Productivity?’ Ecology 82 (9): 2381–96. https://doi.org/10.1890/0012-9658(2001)082%255B2381:WITORB%255D2.0.CO;2.\nNieto, Sebastián, Pedro Flombaum, and Martín F. Garbulsky. 2015. ‘Can Temporal and Spatial NDVI Predict Regional Bird-Species Richness?’ Global Ecology and Conservation 3 (January): 729–35. https://doi.org/10.1016/j.gecco.2015.03.005.\nPigot, Alexander L., Joseph A. Tobias, and Walter Jetz. 2016. ‘Energetic Constraints on Species Coexistence in Birds’. PLOS Biology 14 (3): e1002407. https://doi.org/10.1371/journal.pbio.1002407.\n\n\n\nBBS data (2018): https://www.sciencebase.gov/catalog/item/52b1dfa8e4b0d9b325230cd9\nBBS routes geometry: https://earthworks.stanford.edu/catalog/stanford-vy474dv5024\nNDVI (June 2018): https://neo.gsfc.nasa.gov/view.php?datasetId=MOD_NDVI_M&year=2018\nelevation: https://earthengine.google.com/\nUS boundaries: https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html"
  },
  {
    "objectID": "assignment.html#data-preparation",
    "href": "assignment.html#data-preparation",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "I chose the lower 48 states of USA as the study area because for them the route geometries are publicly available. In the basic dataset of BBS, the geometry is included only for the starting point of the routes, which are on average about 40 km long and therefore the single point does not optimally represent their geometry. Firstly, we need to select the boundary for the study area.\n\n\nShow the code\nus_states = gpd.read_file(\"study_area/cb_2018_us_state_500k.shp\")\nexclude = [\"AK\", \"HI\", \"PR\", \"VI\", \"MP\", \"GU\", \"AS\"] # States outside the area of interest\nus_states = us_states.query(\"STUSPS not in @exclude\") # Accessing the exclude variable outside the query() string\nus_boundary = us_states.dissolve()\n\n# Alternatively you can read it directly from the web:\n# states_url = \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_500k.zip\"\n# r = httpx.get(states_url, verify=False, timeout=None)\n# us_states = gpd.read_file(r.content, engine=\"pyogrio\")\n\n\n\n            \n            \n\n\n\n\n\nNow it is time to load the data about species richness, which consists of the number of species detected at the route. I decided to use data from 2018, which I already prepared in R, and computed species richness for each transect. This data has one geometry column with the coordinates of the starting point of the route. I downloaded the data at the official project site: https://www.sciencebase.gov/catalog/item/52b1dfa8e4b0d9b325230cd9.\n\n\nShow the code\nbbs = gpd.read_file(\"bbs/bbs_routes_SR_2018.gpkg\", engine = \"fiona\")\n\nbbs = bbs.loc[bbs[\"CountryNum\"] == 840] # Select only US transects\nbbs = bbs.loc[bbs[\"StateNum\"] != 3] # Select every state except Alaska\nbbs[\"RTENO\"] = bbs[\"RTENO\"].astype(str).str[3:] # Slice the string (skips the first 3 characters)\n\n\n\n            \n            \n\n\nLet’s load the data for the routes. Unfortunately, the route geometries are often inaccurate and sometimes consists of several fragments, which can cause problems. Therefore, we first need to remove potentially wrong route geometries. The expected route geometry ought to be about 40 km long, so let us filter potential outliers from this value. This cleaning will result in the loss of several hundred datapoints, but we need precise route geometries for data extraction.\n\n\nShow the code\nroutes = gpd.read_file(\"routes/bbsrtsl020.shp\")\n\nroutes = routes.to_crs(epsg=5070) # First set the crs to USA Contiguous Albers Equal Area Conic projection\nroutes[\"calc_length_km\"] = routes.geometry.length / 1000 # Calculating the route length for given geometry\nroutes = routes.loc[routes[\"RTELENG\"] &lt;= 45000,] # Selecting routes under 45 km in length by the dataset measure\nmask = (routes[\"calc_length_km\"] &gt;= 35) & (routes[\"calc_length_km\"] &lt;= 45) # Filter out fragments and over-long routes\nroutes_filtered = routes[mask].copy()\n\nprint(f\"Original routes count: {len(routes)}\")\nprint(f\"Filtered routes count (35-45km): {len(routes_filtered)}\")\nprint(f\"Average route length: {routes_filtered[\"calc_length_km\"].mean():.2f} km\")\n\n\n\n            \n            \n\n\nOriginal routes count: 3062\nFiltered routes count (35-45km): 2570\nAverage route length: 40.14 km\n\n\nLet us perform the spatial join of these two geodataframes. To combine BBS data with routes geometries we use spatial join using “inner” as we need matching values in both tables. I could also join the tables by column “RTENO”, but I want to be sure that the geometries match.\n\n\nShow the code\nbbs = bbs.to_crs(routes_filtered.crs) # Make sure it's in the same coordinate system\nbbs = bbs.drop(columns = \"RTENO\")\n\nbbs[\"geometry\"] = bbs.buffer(400) # Add a small buffer (400 m) to points to account for GPS inaccuracy\n\njoined_data = gpd.sjoin(routes_filtered, bbs, how=\"inner\", predicate=\"intersects\") # Spatial join of bbs points data with the routes geometries\n\nprint(f\"Routes in subset: {len(routes_filtered)}\")\nprint(f\"Routes with data: {len(joined_data[\"RTENO\"].unique())}\")\n\n\n\n            \n            \n\n\nRoutes in subset: 2570\nRoutes with data: 1611\n\n\nThe large drop in the number of routes is due to missing BBS data for the route geometries, which could be due to annual differences in the census - in some years, particular routes are not counted due to weather conditions, terrain obstacles or illness of the observer. However, the number is still very large, so there are probably some other errors and inaccuracies in geometry matching. Now we will do the final preparation of the table for the NDVI and elevation values extraction. We need to get rid of redundant columns, create buffer around the routes, add the route midpoint and create Lat/Lon columns for external API.\n\n\nShow the code\n# Add .copy() here to break the link to joined_data\nbbs_routes = joined_data[[\"RTENO\", \"species_richness\", \"geometry\"]].copy()\n\nbbs_routes[\"buff_3000\"] = bbs_routes.buffer(3000)  # Creating 3 km buffer - buffer choice explained in NDVI extraction part\nbbs_routes[\"route_midpoint\"] = bbs_routes.geometry.interpolate(0.5, normalized=True) # Calculate the midpoint (50% along the line) as a reference geometry\nbbs_routes = bbs_routes.drop(columns = \"geometry\") # We no longer need routes geometry\n\ntemp_gdf = bbs_routes.set_geometry(\"route_midpoint\").to_crs(4326)\nbbs_routes[\"lon\"] = temp_gdf.geometry.x\nbbs_routes[\"lat\"] = temp_gdf.geometry.y\nbbs_routes = bbs_routes.set_geometry(\"buff_3000\")\nbbs_routes.head()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nRTENO\nspecies_richness\nbuff_3000\nroute_midpoint\nlon\nlat\n\n\n\n\n0\n2072\n63\nPOLYGON ((782767.664 1352804.909, 782613.87 13...\nPOINT (799058.576 1358112.615)\n-87.165585\n34.947661\n\n\n2\n2204\n54\nPOLYGON ((911518.247 1350136.581, 911484.644 1...\nPOINT (920348.359 1362929.663)\n-85.828469\n34.882272\n\n\n3\n2073\n65\nPOLYGON ((883221.105 1343004.45, 883424.384 13...\nPOINT (896739.418 1353510.697)\n-86.098287\n34.820791\n\n\n4\n2071\n61\nPOLYGON ((766029.847 1327436.538, 765484.087 1...\nPOINT (776778.876 1331128.227)\n-87.437364\n34.725655\n\n\n5\n2007\n63\nPOLYGON ((869910.569 1340151.781, 869882.814 1...\nPOINT (879587.76 1341112.337)\n-86.300271\n34.726327\n\n\n\n\n\n\n\n\n\n\nThe normalised difference vegetation index (NDVI) indicates vegetation health within raster image pixels by quantifying how much near-infrared (NIR) light the plants reflect. Healthy vegetation reflects a higher proportion of NIR and a lower proportion of red light than stressed vegetation or non-living surfaces with similar visible colours (for example, turf fields). This contrast makes NDVI a practical tool for evaluating how healthy vegetation is in a raster image relative to its surroundings.\nNDVI is calculated for each pixel with the following calculation:\n\\(\\Large NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\)\nThis formula generates a value between -1 and +1. Low reflectance in the red channel and high reflectance in the NIR channel will yield a high NDVI value (healthy vegetation), while the inverse will result in a low NDVI value (unhealthy vegetation). Negative values typically represent non-vegetation such as water or rock.\nThe NDVI raster for the whole area would be very large (probably several Gb) and computationally demanding. Therefore, I decided to choose a different approach and work with compressed 8-bit raster (values from 0 to 255) from NEO (Nasa Earth Observations), which is a 1-month average (June). According to NASA Earth Observations (NEO) (2026), the values contained in this file have been scaled and resampled to support visualisation in NEO and are not suitable for rigorous scientific analysis. However, they may be used for basic assessments and for identifying general trends, which is exactly what we are going to do. We only need to resolve the different scaling of the values. We will extract the mean NDVI values for 3 km buffer. these distances preserve the higher predictive power for covariates in studies of breeding birds (e.g. Byer et al. 2025).\n\n\nShow the code\nndvi = rioxarray.open_rasterio(\"MOD_NDVI_M_2018-06-01_rgb_3600x1800.TIFF\")\n\nndvi_us = ndvi.rio.clip(us_boundary.to_crs(ndvi.rio.crs).geometry) # Matching CRS and cliping to study area\nndvi_us = ndvi_us.drop_vars(\"band\").squeeze() # Getting rid of redundant dimension\nndvi_us = ndvi_us.where(ndvi_us&gt;0) # Filtering null values\n\nprint(f\"Raster CRS: {ndvi_us.rio.crs}\") # Check the CRS to see if it's in degrees (4326) or meters\n\n\n\n            \n            \n\n\nRaster CRS: EPSG:4326\n\n\n\n\nShow the code\n_ = ndvi_us.plot()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nWe can see from the map that there is a very high NDVI on the east coast compared to the west of US. Now we can extract values from the raster for the routes buffer we created earlier. We will extract 1) the mean value of NDVI for each buffer and 2) the standard deviation of NDVI for each buffer. The latter will help estimate the heterogeneity of the vegetation.\n\n\nShow the code\nndvi_us = ndvi_us.rio.reproject(bbs_routes.crs) # Reprojecting to EPSG: 5070\nndvi_extraction = bbs_routes.to_crs(ndvi_us.rio.crs)\n\nzonal_stats = ndvi_us.drop_vars(\"spatial_ref\").xvec.zonal_stats(\n    geometry=ndvi_extraction.buff_3000,\n    x_coords=\"x\",\n    y_coords=\"y\",\n    stats=[\"mean\", \"std\"],\n)\n\n\n\n            \n            \n\n\nLet us check if there are some missing values in the data.\n\n\nShow the code\nndvi = zonal_stats.xvec.to_geodataframe(name=\"NDVI\") # Tranforming zonal statistics into geodataframe\nprint(ndvi.isna().sum()) # Returns the count of NaNs for every column in the dataframe\n\n\n\n            \n            \n\n\ngeometry     0\nindex        0\nNDVI        66\ndtype: int64\n\n\n\n\nShow the code\nndvi_clean = ndvi.dropna(subset=[\"NDVI\"]) # Dropping rows where the NDVI value is missing\n\nndvi_stats = ndvi_clean.reset_index().pivot(\n    index=\"index\", \n    columns=\"zonal_statistics\", \n    values=\"NDVI\"\n) # This creates a table with index as rows and mean/std/min/max as columns\n\nndvi_stats.columns = [\"ndvi_\" + col for col in ndvi_stats.columns] # Add the \"ndvi_\" prefix to the new columns\ngeometries = ndvi_clean[[\"index\", \"geometry\"]].drop_duplicates(\"index\").set_index(\"index\") # Get the unique geometries for each index\nndvi_data = geometries.join(ndvi_stats).reset_index() # Join them together\n\n\n\n            \n            \n\n\nAlthough the NDVI data was retrieved in 8-bit format (0–255), we can standardise it to Z-scores prior to modelling. This process centres the data and removes the original units, ensuring that the coefficients in the models are not biassed by the original bit-depth of the satellite imagery.\n\n\nShow the code\n# Standardize the Mean (Relative Productivity)\n# (Value - average of all routes) / standard deviation of all routes\nndvi_data[\"ndvi_mean_z\"] = (ndvi_data[\"ndvi_mean\"] - ndvi_data[\"ndvi_mean\"].mean()) / ndvi_data[\"ndvi_mean\"].std()\n\n# Standardize the Std (Relative Heterogeneity)\n# We treat the \"std\" as its own variable and standardize IT.\nndvi_data[\"ndvi_heterog_z\"] = (ndvi_data[\"ndvi_std\"] - ndvi_data[\"ndvi_std\"].mean()) / ndvi_data[\"ndvi_std\"].std()\n\n\n\n            \n            \n\n\n\n\nShow the code\nndvi_to_join = ndvi_data[[\"ndvi_mean_z\", \"ndvi_heterog_z\", \"geometry\"]]\n\nbuffer = bbs_routes.set_geometry(\"buff_3000\")\n\n# Perform the Spatial Join\n# \"predicate=inner\" ensures the polygons must overlap/be the same\n# \"how=inner\" keeps only rows that exist in both sets\nbbs_ndvi = gpd.sjoin(\n    buffer, \n    ndvi_to_join, \n    how=\"inner\", \n    predicate=\"intersects\"\n)\n\nbbs_ndvi = bbs_ndvi.drop(columns=\"index_right\") # Dropping index_right\ndata = bbs_ndvi.drop_duplicates(subset=[\"RTENO\"]) # Check for duplicates\ndata.info()\n\n\n\n            \n            \n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 1592 entries, 0 to 3730\nData columns (total 8 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   RTENO             1592 non-null   int64   \n 1   species_richness  1592 non-null   int64   \n 2   buff_3000         1592 non-null   geometry\n 3   route_midpoint    1592 non-null   geometry\n 4   lon               1592 non-null   float64 \n 5   lat               1592 non-null   float64 \n 6   ndvi_mean_z       1592 non-null   float64 \n 7   ndvi_heterog_z    1592 non-null   float64 \ndtypes: float64(4), geometry(2), int64(2)\nmemory usage: 111.9 KB\n\n\n\n\nShow the code\nm = data.explore(\n    column=\"ndvi_mean_z\", \n    cmap=\"RdYlGn\",\n    marker_kwds={'radius':5},\n    vmin=-2.5, \n    vmax=2.5, \n    legend=True,\n    tooltip=[\"ndvi_mean_z\"], # Hover to see raw vs z-score\n    tiles=\"CartoDB positron\",\n    popup=True # Click to see all data for that route\n)\nm\n\n\n\n            \n            \n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThis map displays the relative differences in the NDVI standardised in the US. A visible challenge for our spatial model is the high density and overlap of the routes in the Northeast. This concentration introduces sampling bias (over-representing certain environments) and pseudo replication, where overlapping buffers sample nearly identical pixels, violating the assumption of independent observations.\nTo mitigate this, I implemented a spatial thinning procedure using the function thin_spatial_geopandas. This algorithm utilises a spatial index (sindex) to efficiently identify route buffers within a 1 m threshold and randomly selects one representative route from overlapping pairs to be retained, while discarding the rest. This ensures a more geographically balanced dataset for subsequent analysis.\n\n\nShow the code\ndef thin_spatial_geopandas(data, distance_threshold=1):\n    # 1. Ensure we are working with a copy and shuffle for randomness\n    gdf_temp = data.copy().sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    keep_indices = []\n    discard_indices = []\n    processed = np.zeros(len(gdf_temp), dtype=bool)\n\n    # 2. Build a spatial index\n    sindex = gdf_temp.sindex\n\n    for i in range(len(gdf_temp)):\n        if processed[i]:\n            continue\n        \n        # Keep this route\n        keep_indices.append(i)\n        processed[i] = True\n        \n        # Find all neighbors within the distance threshold\n        current_geom = gdf_temp.geometry.iloc[i]\n        # query returns indices of geometries whose bounding box intersects the search area\n        # Then we filter by actual distance\n        potential_neighbors = sindex.query(current_geom.buffer(distance_threshold))\n        \n        for neighbor_idx in potential_neighbors:\n            if not processed[neighbor_idx] and neighbor_idx != i:\n                # Double check actual distance (since query uses bounding boxes)\n                if current_geom.distance(gdf_temp.geometry.iloc[neighbor_idx]) &lt; distance_threshold:\n                    discard_indices.append(neighbor_idx)\n                    processed[neighbor_idx] = True\n                    \n    return gdf_temp.iloc[keep_indices], gdf_temp.iloc[discard_indices]\n\n# Run the thinning (make sure your CRS is in meters, e.g., EPSG:5070)\ndf_thinned, df_removed = thin_spatial_geopandas(data, distance_threshold=1)\n\nprint(f\"Routes kept: {len(df_thinned)}\")\nprint(f\"Routes removed: {len(df_removed)}\")\n\n\n\n            \n            \n\n\nRoutes kept: 1442\nRoutes removed: 150\n\n\n\n\nShow the code\nbbs_ndvi = df_thinned.drop(columns = \"buff_3000\") # We no longer need buffer geometry\nbbs_ndvi = bbs_ndvi.set_geometry(\"route_midpoint\")\nbbs_ndvi.info()\n\n\n\n            \n            \n\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nIndex: 1442 entries, 0 to 1591\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   RTENO             1442 non-null   int64   \n 1   species_richness  1442 non-null   int64   \n 2   route_midpoint    1442 non-null   geometry\n 3   lon               1442 non-null   float64 \n 4   lat               1442 non-null   float64 \n 5   ndvi_mean_z       1442 non-null   float64 \n 6   ndvi_heterog_z    1442 non-null   float64 \ndtypes: float64(4), geometry(1), int64(2)\nmemory usage: 90.1 KB\n\n\n\n\n\nTo get the elevation data, I integrated topographic data from the NASA Digital Elevation Model (NASADEM). Since the Breeding Bird Survey data do not inherently contain altitude information, I used the Google Earth Engine (GEE) API to perform cloud-based spatial join operations.\nThe extraction process involved several technical steps:\nFeature Transformation: Local coordinate pairs (Latitude/Longitude) for each survey route midpoint were converted into ee.Feature objects to make them compatible with the GEE spatial engine. - High-Resolution Sampling: I accessed the NASADEM Global 30m dataset. Using a 30-metre sampling scale, the extraction matched the native resolution of the satellite sensor, ensuring that the elevation value assigned to each route accurately reflects the local terrain. - Data Integration: The extracted values were pulled from the cloud using the geemap library and merged back into the primary research DataFrame using the unique route identifier (RTENO).\n\n\nShow the code\n# 1. Initialize Earth Engine Project ID\ntry:\n    ee.Initialize(project=\"my-project-ee-470714\")\nexcept Exception as e:\n    ee.Authenticate()\n    ee.Initialize(project=\"my-project-ee-470714\")\n\n# 2. Convert DataFrame rows into GEE Features\nfeatures = []\nfor index, row in bbs_ndvi.iterrows():\n    # GEE uses [Longitude, Latitude]\n    point = ee.Geometry.Point([row[\"lon\"], row[\"lat\"]])\n    feat = ee.Feature(point, {\"RTENO\": row[\"RTENO\"]})\n    features.append(feat)\n\npoints_ee = ee.FeatureCollection(features)\n\n# 3. Load the NASADEM Global 30m dataset\nnasadem = ee.Image(\"NASA/NASADEM_HGT/001\").select(\"elevation\")\n\n# 4. Sample the image at road midpoints\nsampled_points = nasadem.sampleRegions(\n    collection=points_ee,\n    properties=[\"RTENO\"],\n    scale=30\n)\n\n# 5. Convert back to a Pandas DataFrame\n# 5. Convert back to a Pandas DataFrame (The Direct Way)\ninfo = sampled_points.getInfo()\nfeatures = info['features']\ndict_list = [f['properties'] for f in features]\ndf_elev = pd.DataFrame(dict_list)\n# 6. Merge the elevation into final DataFrame\ndata_final = bbs_ndvi.merge(df_elev[[\"RTENO\", \"elevation\"]], on=\"RTENO\", how=\"left\")\n\nprint(\"Elevation successfully extracted from GEE!\")\n\n\n\n            \n            \n\n\nElevation successfully extracted from GEE!\n\n\n\n\nShow the code\ndata_final[\"elevation\"].describe()\n\n\n\n            \n            \n\n\ncount    1442.000000\nmean      592.497226\nstd       648.428474\nmin        -1.000000\n25%       162.000000\n50%       314.000000\n75%       816.750000\nmax      3289.000000\nName: elevation, dtype: float64\n\n\n\n\nShow the code\n# Save to a GeoPackage\n# Pass the column names as a list\ndata_final.to_file(\"assignment_data.gpkg\", layer=\"bbs_routes\", driver=\"GPKG\")\n\n\n\n            \n            \n\n\n\n\nShow the code\ndata = gpd.read_file(\"assignment_data.gpkg\")\ndata.head()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nRTENO\nspecies_richness\nlon\nlat\nndvi_mean_z\nndvi_heterog_z\nelevation\ngeometry\n\n\n\n\n0\n60130\n26\n-103.354305\n32.340717\n-2.006735\n-0.573757\n1073\nPOINT (-687991.066 1056389.17)\n\n\n1\n72016\n65\n-78.818844\n41.692045\n1.028855\n-0.141942\n508\nPOINT (1411937.291 2204439.401)\n\n\n2\n91036\n55\n-90.357021\n44.651087\n0.146810\n0.078148\n369\nPOINT (446389.92 2420135.718)\n\n\n3\n34068\n47\n-89.311522\n41.466289\n1.044674\n-0.629305\n221\nPOINT (553981.276 2070695.428)\n\n\n4\n91023\n56\n-89.648401\n45.779301\n0.873833\n-0.629305\n502\nPOINT (493986.861 2548870.791)"
  },
  {
    "objectID": "assignment.html#spatial-autocorrelation",
    "href": "assignment.html#spatial-autocorrelation",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "With the data prepared, we first evaluate the extent of spatial dependency in bird species richness using a three-pronged diagnostic approach: - Moran plot - to see the global degree of clustering - Correlogram - to see how the spatial autocorrelation changes with larger neighbourhoods - LISA (Local Indicators of Spatial Association) - to see distribution of significantly spatialy autocorrelated values.\n\n\nShow the code\n# 1. Prepare standardized variables and spatial lags\nknn5 = graph.Graph.build_knn(data, k=5)\nknn5 = knn5.transform(\"R\")\n# Standardize richness (Z-score)\ndata[\"richness_std\"] = (data[\"species_richness\"] - data[\"species_richness\"].mean()) / data[\"species_richness\"].std()\n# Calculate spatial lag using the KNN5 weights you already built\ndata[\"richness_lag\"] = knn5.lag(data[\"richness_std\"])\n# Calculate Moran's I\nmoran = esda.moran.Moran(data['species_richness'], knn5)\n# Calculate correlogram\nk = [5, 10, 25, 50, 75, 100, 500, 1000]\ncorrelogram = esda.correlogram(\n  geometry=data.representative_point(),\n  variable=data[\"species_richness\"],\n  support=k,\n  distance_type=\"knn\",\n)\n\n# 2. Calculate LISA (Local Indicators of Spatial Association)\n# We use .to_W() because Moran_Local expects a weights object\nlisa = esda.moran.Moran_Local(data[\"species_richness\"], knn5.to_W())\n\n# 3. Setup the 3-panel figure\nf, ax = plt.subplots(1, 3, figsize=(24,7))\n\n# --- PANEL 1: MORAN SCATTERPLOT ---\nsns.regplot(\n    x=\"richness_std\",\n    y=\"richness_lag\",\n    data=data,\n    marker=\".\",\n    scatter_kws={\"alpha\": 0.3, \"color\": \"steelblue\"},\n    line_kws=dict(color=\"lightcoral\"),\n    ax=ax[0]\n)\nax[0].set_aspect(\"equal\")\nax[0].axvline(0, c=\"black\", alpha=0.5, linewidth=1)\nax[0].axhline(0, c=\"black\", alpha=0.5, linewidth=1)\nax[0].set_title(f\"Moran Plot\\n\\nMoran's I: {moran.I:.3f} (p-value: {moran.p_sim:.3f})\", fontsize=14)\nax[0].set_xlabel(\"Species Richness (Standardized)\")\nax[0].set_ylabel(\"Spatial Lag (Standardized)\")\n\n# Add Quadrant Labels\nax[0].text(0.95, 0.95, \"HH\", transform=ax[0].transAxes, ha=\"right\", va=\"top\", fontweight=\"bold\")\nax[0].text(0.95, 0.05, \"HL\", transform=ax[0].transAxes, ha=\"right\", va=\"bottom\", fontweight=\"bold\")\nax[0].text(0.05, 0.95, \"LH\", transform=ax[0].transAxes, ha=\"left\", va=\"top\", fontweight=\"bold\")\nax[0].text(0.05, 0.05, \"LL\", transform=ax[0].transAxes, ha=\"left\", va=\"bottom\", fontweight=\"bold\")\n\n# --- PANEL 2: LISA CLUSTER MAP ---\n_ = lisa.plot(data, crit_value=0.05, ax=ax[1], alpha=0.6, markersize=20) \nax[1].set_title(\"LISA Cluster Map (p &lt; 0.05)\", fontsize=14)\ncontextily.add_basemap(ax[1], crs=data.crs.to_string(), source=contextily.providers.CartoDB.PositronNoLabels)\n\n# --- PANEL 3: CORRELOGRAM ---\n# Plotting the I values against the support (K neighbors)\nax[2].plot(k, correlogram.I, marker=\"o\", linestyle=\"-\", color=\"steelblue\")\nax[2].axhline(0, color=\"black\", linestyle=\"--\", alpha=0.5)\nax[2].set_title(\"Spatial Correlogram\", fontsize=14)\nax[2].set_xlabel(\"Number of Nearest Neighbors (K)\")\nax[2].set_ylabel(\"Moran's I\")\nax[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nThe Global Moran’s I value confirms a strong, positive spatial autocorrelation, indicating that the species richness is geographically clustered. The LISA map reveals a distinct East-West dichotomy: the Eastern US is dominated by biodiversity hotspots (High-High), likely driven by higher precipitation and vegetation density, while the arid West exhibits extensive coldspots (Low-Low).\nAn interesting anomaly is the coldspot cluster in Florida. While Florida is ecologically rich in wetlands, so I would not expect a clustering of lower species richness values.\nFinally, the spatial correlogram demonstrates that spatial influence is strongest at small scales but drops significantly until the neighbourhood size reaches approximately 100 neighbours. This inflexion point suggests that the ecological processes driving bird richness operate within a specific regional range."
  },
  {
    "objectID": "assignment.html#linear-regression",
    "href": "assignment.html#linear-regression",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "Now we can continue with linear regression. We already know, that there is a high spatial autocorrelation, therefore our ultimate goal is geographically weighted linear regression (GWR), but first we will build simple OLS model and identify the residuals. However this model will already consist of two spatial predictors - longitude and latitude.\n\n\nShow the code\n# Define our dependent variable (target) and independent variables (predictors)\ndependent = \"species_richness\"\nindependents = [\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\", \"lon\", \"lat\"]\n\n# Construct the formula string: \"species_richness ~ ndvi_mean_z + ndvi_heterog_z + elevation\"\nformula = f\"{dependent} ~ {\" + \".join(independents)}\"\nprint(f\"OLS Formula: {formula}\")\n\n\n\n            \n            \n\n\nOLS Formula: species_richness ~ ndvi_mean_z + ndvi_heterog_z + elevation + lon + lat\n\n\n\n\nShow the code\n# Fit the Ordinary Least Squares (OLS) model\nols = smf.ols(formula, data=data).fit()\n\nols.summary()\n\n\n\n            \n            \n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nspecies_richness\nR-squared:\n0.329\n\n\nModel:\nOLS\nAdj. R-squared:\n0.327\n\n\nMethod:\nLeast Squares\nF-statistic:\n141.1\n\n\nDate:\nFri, 13 Feb 2026\nProb (F-statistic):\n6.76e-122\n\n\nTime:\n14:09:35\nLog-Likelihood:\n-5542.3\n\n\nNo. Observations:\n1442\nAIC:\n1.110e+04\n\n\nDf Residuals:\n1436\nBIC:\n1.113e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n51.1524\n3.096\n16.521\n0.000\n45.079\n57.226\n\n\nndvi_mean_z\n5.8241\n0.449\n12.976\n0.000\n4.944\n6.705\n\n\nndvi_heterog_z\n1.3671\n0.313\n4.368\n0.000\n0.753\n1.981\n\n\nelevation\n-0.0015\n0.001\n-2.415\n0.016\n-0.003\n-0.000\n\n\nlon\n0.1071\n0.029\n3.693\n0.000\n0.050\n0.164\n\n\nlat\n0.3419\n0.066\n5.201\n0.000\n0.213\n0.471\n\n\n\n\n\n\n\n\nOmnibus:\n10.586\nDurbin-Watson:\n1.967\n\n\nProb(Omnibus):\n0.005\nJarque-Bera (JB):\n11.957\n\n\nSkew:\n0.141\nProb(JB):\n0.00253\n\n\nKurtosis:\n3.346\nCond. No.\n9.16e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 9.16e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe Global OLS model indicates that primary productivity (ndvi_mean_z) is the dominant driver of bird species richness across the contiguous US and the model explains about one third of the data (R2=0.329). Bird diversity slightly decreases with higher elevation and increases with vegetation heterogeneity. Longitude and latitude have very small explanatory power. Furthermore, the OLS model assumes these relationships are constant across the continent. To investigate whether these drivers vary geographically—for instance, if NDVI is more important in the arid West than the humid East—I will now proceed to Geographically Weighted Regression (GWR). But first we take a look at the spatial perspective of the OLS model.\n\n\nShow the code\npredicted = ols.predict(data)\ndata[\"residual\"] = ols.resid\nmax_residual = ols.resid.abs().max()\n\nf, axs = plt.subplots(1, 3, figsize=(24, 7))\ndata.plot(\n    predicted, legend=True, cmap=\"coolwarm\", vmin=0, vmax=100, ax=axs[0], alpha = 0.8, markersize=30\n)\ndata.plot(\n    \"species_richness\", legend=True, cmap=\"coolwarm\", vmin=0, vmax=100, ax=axs[1], alpha = 0.8, markersize=30\n)\n\ndata.plot(\n    \"residual\", legend=True, cmap=\"RdBu\", vmin=-max_residual, vmax=max_residual, ax=axs[2], alpha = 0.8, markersize=30\n)\n\nres_moran = esda.moran.Moran(data[\"residual\"], knn5)\n\n\naxs[0].set_title(\"OLS prediction\", fontsize=14)\naxs[1].set_title(\"Actual results\", fontsize=14)\naxs[2].set_title(f\"Residuals\\n\\nMoran's I: {res_moran.I:.3f} (p-value: {res_moran.p_sim:.3f})\", fontsize=14)\n\naxs[0].set_axis_off()\naxs[1].set_axis_off()\naxs[2].set_axis_off()\nplt.tight_layout()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nIn the first map we can observe that the model predicts the expected difference in the species richness between East and West of US. Hence, it both overestimates and underestimates the predictions in some areas, because our data (middle map) shows more diverse pattern. The map on the right present’s residuals, locations where models fail to explain our dependent variable. You can spot the blue areas which have higher species richness than expected by the model, while the red areas show the opposite (including Florida, which has lower species diversity than expected). Moran’s I indicates that residuals form spatial clusters.\n\n\n\n\n\nShow the code\n# 1. Define Dependent (y) and Independent (X) variables\n# We reshape y to (-1, 1) to ensure it is a column vector for matrix math\ny = data[\"species_richness\"].values.reshape((-1, 1))\n\n# We select our environmental predictors\n# We exclude lon/lat from X because they are used in \"coords\" instead\nX = data[[\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\"]].values\n\n# 2. Define the spatial coordinates\ncoords = data.centroid.get_coordinates().values\n\n\n\n            \n            \n\n\n\n\nShow the code\n# Initialize the bandwidth selector\nbw_selector = Sel_BW(coords, y, X, fixed=False, multi=False)\n\n# Search for the optimal bandwidth (number of nearest neighbors)\nopt_bw = bw_selector.search()\n\nprint(f\"Optimal bandwidth (neighbors): {opt_bw}\")\n\n\n\n            \n            \n\n\nOptimal bandwidth (neighbors): 80.0\n\n\n\n\nShow the code\n# Fit the model\ngwr_model = GWR(coords, y, X, opt_bw).fit()\n\n# View the global summary of the local models\ngwr_model.summary()\n\n\n\n            \n            \n\n\n===========================================================================\nModel type                                                         Gaussian\nNumber of observations:                                                1442\nNumber of covariates:                                                     4\n\nGlobal Regression Results\n---------------------------------------------------------------------------\nResidual sum of squares:                                         188213.517\nLog-likelihood:                                                   -5558.494\nAIC:                                                              11124.988\nAICc:                                                             11127.030\nBIC:                                                             177753.813\nR2:                                                                   0.314\nAdj. R2:                                                              0.313\n\nVariable                              Est.         SE  t(Est/SE)    p-value\n------------------------------- ---------- ---------- ---------- ----------\nX0                                  54.352      0.458    118.587      0.000\nX1                                   7.121      0.383     18.574      0.000\nX2                                   1.381      0.313      4.420      0.000\nX3                                  -0.001      0.001     -2.067      0.039\n\nGeographically Weighted Regression (GWR) Results\n---------------------------------------------------------------------------\nSpatial kernel:                                           Adaptive bisquare\nBandwidth used:                                                      80.000\n\nDiagnostic information\n---------------------------------------------------------------------------\nResidual sum of squares:                                         122138.628\nEffective number of parameters (trace(S)):                          160.282\nDegree of freedom (n - trace(S)):                                  1281.718\nSigma estimate:                                                       9.762\nLog-likelihood:                                                   -5246.719\nAIC:                                                              10816.001\nAICc:                                                             10856.906\nBIC:                                                              11666.566\nR2:                                                                   0.555\nAdjusted R2:                                                          0.499\nAdj. alpha (95%):                                                     0.001\nAdj. critical t value (95%):                                          3.234\n\nSummary Statistics For GWR Parameter Estimates\n---------------------------------------------------------------------------\nVariable                   Mean        STD        Min     Median        Max\n-------------------- ---------- ---------- ---------- ---------- ----------\nX0                       55.634      9.885     19.849     55.695     85.198\nX1                        4.078      5.228    -14.156      4.573     16.293\nX2                        1.041      2.064     -6.011      1.245      6.923\nX3                        0.002      0.037     -0.083     -0.002      0.200\n===========================================================================\n\n\n\nThe GWR model increased the explained variability to 55 % and we can now do a visual comparison with the OLS model and actual data.\n\n\nShow the code\n# Create a copy to store results\ndata_results = data.copy()\n\n# Add Local R2 (Where does the model perform best?)\ndata_results['gwr_R2'] = gwr_model.localR2\n\n# Add Local Coefficients\n# Since X had [ndvi_mean_z, ndvi_heterog_z, elevation], the params match that order\ndata_results['coef_ndvi'] = gwr_model.params[:, 0]\ndata_results['coef_hetero'] = gwr_model.params[:, 1]\ndata_results['coef_elev'] = gwr_model.params[:, 2]\n\n# Add T-values (to check local significance)\n# A t-value &gt; 1.96 or &lt; -1.96 means the relationship is significant at 95%\ndata_results['t_ndvi'] = gwr_model.tvalues[:, 0]\n\n\n\n            \n            \n\n\n\n\nShow the code\n# 1. Define the maximum richness for a consistent scale\nv_min = 0\nv_max = 100 \n\nfig, axs = plt.subplots(1, 3, figsize=(24, 7))\n\n# --- Plot 1: OLS Prediction ---\n# We use the global OLS model to predict based on the dataframe\ndata.plot(\n    ols.predict(data), \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[0], markersize=40\n)\naxs[0].set_title(\"OLS Prediction (Global Model)\", fontsize=14)\n\n# --- Plot 2: GWR Prediction ---\n# 'predy' contains the local predictions from the GWR model\ndata.plot(\n    gwr_model.predy.flatten(), \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[1], markersize=40\n)\naxs[1].set_title(\"GWR Prediction (Local Model)\", fontsize=14)\n\n# --- Plot 3: Actual Data ---\n# The ground truth: your 'species_richness' column\ndata.plot(\n    \"species_richness\", \n    legend=True, cmap=\"coolwarm\", vmin=v_min, vmax=v_max, ax=axs[2], markersize=40\n)\naxs[2].set_title(\"Actual Species Richness\", fontsize=14)\n\n# Cleanup\nfor ax in axs:\n    ax.set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nAs we can see from the maps, the GWR model better explains the variability on the actual data further highliting the contrast between East and West, however still fails to predicts very high or very low values in some areas - to look at them specifically, we can examine the local Beta coefficients.\n\n\nShow the code\n# 1. Define significance threshold\nsig95 = gwr_model.adj_alpha[1]\ncritical_t = gwr_model.critical_tval(alpha=sig95)\ncritical_t\nsignificant = np.abs(gwr_model.tvalues) &gt; critical_t\n\n# 2. Get the actual number of variables from the model\nnum_vars = gwr_model.params.shape[1]\n\n# 3. Build var_names safely\n# 'independents' usually includes the target or coords, so we slice it \n# to match the X matrix you fed into the GWR\n# If your X was [ndvi_mean_z, ndvi_heterog_z, elevation], then:\nactual_predictor_names = [\"ndvi_mean_z\", \"ndvi_heterog_z\", \"elevation\"]\nvar_names = [\"Intercept\"] + actual_predictor_names\n\n# 4. Create the grid\ncols = 2\nrows = (num_vars + cols - 1) // cols \n\nfig, axs = plt.subplots(rows, cols, figsize=(12, rows * 5))\naxs = axs.flatten()\n\nfor i in range(num_vars):\n    # Plot the local coefficients\n    data.plot(\n        gwr_model.params[:, i], \n        cmap=\"coolwarm\", \n        ax=axs[i], \n        markersize=15, # Increased size for better visibility\n        legend=True,\n        legend_kwds={\"shrink\": 0.5}\n    )\n    \n    # Significance Mask\n    non_sig_mask = ~significant[:, i]\n    if non_sig_mask.any():\n        data[non_sig_mask].plot(\n            color=\"white\", \n            ax=axs[i], \n            alpha=0.7, \n            markersize=15\n        )\n    \n    # Check if we have a name for this index to avoid IndexError\n    title_name = var_names[i] if i &lt; len(var_names) else f\"Variable {i}\"\n    axs[i].set_title(f\"Local Coef: {title_name}\", fontsize=12)\n    axs[i].set_axis_off()\n\n# Hide any extra axes\nfor j in range(i + 1, len(axs)):\n    axs[j].set_axis_off()\n\nplt.tight_layout()\nplt.show()\n\n\n\n            \n            \n\n\n\n\n\n\n\n\n\nThe intercept coefficient shows the species richness that was not explained by the model. We can see clusters of species poor routes in the rain shadow of the Rocky Mountains and Great Basin, while biodiversity hotpost are situated mostly in the area of Great Lakes. These patterns, not captured by the model, could be also explained by combination of factors like regional history, presence of nature protective areas and human influence. Relative NDVI significantly drives bird diversity in the arid west, especially in desert habitats, where only a slight increase in vegetation can result in a more diverse bird community. The heterogeneity of the vegetation is significant only in the Great Basin and has not a very strong effect. Finally, in Florida, elevation implies the explanation of species richness, but this is very probably not the right prediction as Florida has very low elevation heterogeneity and it only significantly differs from the other states in the average elevation, but this is surely not the driver of bird diversity in this area. The generally lower species diversity of Florida Peninsula in the model is driven by some factors which were not included - maybe high human population density or climate unstability."
  },
  {
    "objectID": "assignment.html#conclusion",
    "href": "assignment.html#conclusion",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "This study confirms that bird species richness is driven by spatially varying processes. Global Moran’s I and LISA maps reveal a sharp East-West dichotomy, with the East acting as a biodiversity hotspot and the West as a coldspot. While OLS models capture broad trends, GWR significantly improves accuracy by accounting for local “non-stationarity.”\nLocal coefficients show that NDVI is a vital driver in the arid West, where marginal greenness spikes diversity. Despite the temporarly limiting data and proxy variable for NDVI, the model sufficiently revealed the expected SR~NDVI relationship, but should be treated as explorational with higly limiting significance of the results."
  },
  {
    "objectID": "assignment.html#references",
    "href": "assignment.html#references",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "Brüggeshemke, Jonas, and Thomas Fartmann. 2025. ‘Predicting Species Richness and Abundance of Breeding Birds by Remote‐sensing and Field‐survey Data’. Ecological Solutions and Evidence 6 (4): e70170. https://doi.org/10.1002/2688-8319.70170.\nByer, Nathan W., Remington J. Moll, Timothy J. Krynak, et al. 2025. ‘Breeding Bird Sensitivity to Urban Habitat Quality Is Multi‐scale and Strongly Dependent on Migratory Behavior’. Ecological Applications 35 (1): e3087. https://doi.org/10.1002/eap.3087.\nDi Cecco, Grace J., Sara J. Snell Taylor, Ethan P. White, and Allen H. Hurlbert. 2022. ‘More Individuals or Specialized Niches? Distinguishing Support for Hypotheses Explaining Positive Species–Energy Relationships’. Journal of Biogeography 49 (9): 1629–39. https://doi.org/10.1111/jbi.14459.\nFleischmann, M. (2024) A course on Spatial Data Science. Available at: https://martinfleischmann.net/sds/.\nHawkins, Bradford A., Eric E. Porter, and José Alexandre Felizola Diniz-Filho. 2003. ‘Productivity and History as Predictors of the Latitudinal Diversity Gradient of Terrestrial Birds’. Ecology 84 (6): 1608–23. https://doi.org/10.1890/0012-9658(2003)084%255B1608:PAHAPO%255D2.0.CO;2.\nMittelbach, Gary G., Christopher F. Steiner, Samuel M. Scheiner, et al. 2001. ‘What Is the Observed Relationship Between Species Richness and Productivity?’ Ecology 82 (9): 2381–96. https://doi.org/10.1890/0012-9658(2001)082%255B2381:WITORB%255D2.0.CO;2.\nNieto, Sebastián, Pedro Flombaum, and Martín F. Garbulsky. 2015. ‘Can Temporal and Spatial NDVI Predict Regional Bird-Species Richness?’ Global Ecology and Conservation 3 (January): 729–35. https://doi.org/10.1016/j.gecco.2015.03.005.\nPigot, Alexander L., Joseph A. Tobias, and Walter Jetz. 2016. ‘Energetic Constraints on Species Coexistence in Birds’. PLOS Biology 14 (3): e1002407. https://doi.org/10.1371/journal.pbio.1002407."
  },
  {
    "objectID": "assignment.html#data-sources",
    "href": "assignment.html#data-sources",
    "title": "Spatial Data Science Assignment",
    "section": "",
    "text": "BBS data (2018): https://www.sciencebase.gov/catalog/item/52b1dfa8e4b0d9b325230cd9\nBBS routes geometry: https://earthworks.stanford.edu/catalog/stanford-vy474dv5024\nNDVI (June 2018): https://neo.gsfc.nasa.gov/view.php?datasetId=MOD_NDVI_M&year=2018\nelevation: https://earthengine.google.com/\nUS boundaries: https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html"
  }
]